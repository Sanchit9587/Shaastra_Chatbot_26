{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanchit9587/Shaastra_Chatbot_26/blob/main/RAG_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DFq05i9faZn"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers chromadb PyPDF2 transformers accelerate huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import chromadb\n",
        "from chromadb import PersistentClient\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "RWTiqd7XeEFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup the API keys here\n",
        "\n",
        "HF_TOKEN = \"YOUR_HF_API_KEY\"  # <<-- put your HF access token here\n",
        "\n",
        "if \"YOUR_HF_TOKEN_HERE\" in HF_TOKEN:\n",
        "    print(\"[WARN] Please set HF_TOKEN at the top of this cell if the model is gated.\")\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"/content/.cache/huggingface\"\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN"
      ],
      "metadata": {
        "id": "r4AR036oeHEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining models and dbs we will be using\n",
        "\n",
        "# Embedding model (free & local)\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Gemma model (local in Colab)\n",
        "GEMMA_MODEL_NAME = \"google/gemma-7b-it\"\n",
        "\n",
        "# Chroma config\n",
        "CHROMA_PATH = \"/content/chroma_db\"\n",
        "COLLECTION_NAME = \"pdf_docs_collection\"\n",
        "DOCS_FOLDER = \"/content/Documents\"   # Put your PDFs here\n"
      ],
      "metadata": {
        "id": "u03P95qBeJsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the embedding model\n",
        "\n",
        "print(f\"[INFO] Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(\"[INFO] Embedding model loaded.\")\n"
      ],
      "metadata": {
        "id": "rqPvWICleLXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Gemma Locally because the HF API just does not work\n",
        "\n",
        "print(f\"[INFO] Loading Gemma model: {GEMMA_MODEL_NAME}\")\n",
        "if \"YOUR_HF_TOKEN_HERE\" in HF_TOKEN:\n",
        "    print(\"[WARN] HF_TOKEN not set. Model download may fail if the model is gated.\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    GEMMA_MODEL_NAME,\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Use bfloat16 for efficiency; device_map=\"auto\" to spread across GPU/CPU if needed\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    GEMMA_MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"[INFO] Gemma model loaded.\")"
      ],
      "metadata": {
        "id": "twyCWDtieNEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Just setting up few things\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    \"\"\"Very simple sentence splitter for English-like text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Embed texts using SentenceTransformer (local, free).\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "    embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
        "    return embeddings.tolist()\n",
        "\n",
        "\n",
        "def cosine_similarity(vec_a: List[float], vec_b: List[float]) -> float:\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    a = np.array(vec_a, dtype=np.float32)\n",
        "    b = np.array(vec_b, dtype=np.float32)\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-10\n",
        "    return float(np.dot(a, b) / denom)"
      ],
      "metadata": {
        "id": "NHYuoVP6eQIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading in then PDF\n",
        "\n",
        "def load_pdfs_from_folder(folder_path: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Load all .pdf files from given folder.\n",
        "    Returns list of (filename, extracted_text).\n",
        "    \"\"\"\n",
        "    docs: List[Tuple[str, str]] = []\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"[WARN] Folder does not exist: {folder_path}\")\n",
        "        return docs\n",
        "\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.lower().endswith(\".pdf\"):\n",
        "            full_path = os.path.join(folder_path, fname)\n",
        "            try:\n",
        "                reader = PdfReader(full_path)\n",
        "                pages_text = []\n",
        "                for page in reader.pages:\n",
        "                    t = page.extract_text() or \"\"\n",
        "                    pages_text.append(t)\n",
        "                text = \"\\n\".join(pages_text)\n",
        "                docs.append((fname, text))\n",
        "                print(f\"[INFO] Loaded PDF: {fname} ({len(reader.pages)} pages)\")\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed to read {fname}: {e}\")\n",
        "    return docs"
      ],
      "metadata": {
        "id": "uZQsqGjVeR4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunker\n",
        "\n",
        "def recursive_character_split(\n",
        "    text: str,\n",
        "    chunk_size: int = 2000,\n",
        "    chunk_overlap: int = 300,\n",
        "    separators: List[str] = None,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Recursive character-based splitter similar to LangChain's RecursiveCharacterTextSplitter.\n",
        "\n",
        "    Strategy:\n",
        "    - Start with big separators: paragraphs \"\\\\n\\\\n\"\n",
        "    - Then lines \"\\\\n\"\n",
        "    - Then sentence-ish \". \"\n",
        "    - Then spaces \" \"\n",
        "    - Finally raw characters \"\"\n",
        "    - For any segment longer than `chunk_size`, recursively split with the next separator.\n",
        "    - After we get small segments, we merge them into final chunks with `chunk_overlap`.\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \"]\n",
        "\n",
        "    def _split(text: str, sep_index: int) -> List[str]:\n",
        "        # If text already small enough, stop splitting\n",
        "        if len(text) <= chunk_size:\n",
        "            return [text]\n",
        "\n",
        "        # If we've exhausted separators, hard-cut by characters\n",
        "        if sep_index >= len(separators):\n",
        "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "        sep = separators[sep_index]\n",
        "        # If separator is empty string, treat as char-level split\n",
        "        if sep == \"\":\n",
        "            # already at character level; just hard split\n",
        "            return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "        parts = text.split(sep)\n",
        "        result: List[str] = []\n",
        "\n",
        "        for i, part in enumerate(parts):\n",
        "            # Re-add the separator we split on (except before first part)\n",
        "            if i < len(parts) - 1:\n",
        "                candidate = part + sep\n",
        "            else:\n",
        "                candidate = part\n",
        "\n",
        "            if len(candidate) <= chunk_size:\n",
        "                result.append(candidate)\n",
        "            else:\n",
        "                # Too big, go one level deeper with a smaller separator\n",
        "                result.extend(_split(candidate, sep_index + 1))\n",
        "\n",
        "        return result\n",
        "\n",
        "    # First pass: recursively split until every piece <= chunk_size\n",
        "    raw_pieces = [p for p in _split(text, 0) if p and p.strip()]\n",
        "\n",
        "    # Second pass: merge pieces into chunks with overlap\n",
        "    chunks: List[str] = []\n",
        "    current = \"\"\n",
        "\n",
        "    for piece in raw_pieces:\n",
        "        piece = piece.strip()\n",
        "        if not piece:\n",
        "            continue\n",
        "\n",
        "        # If current is empty, start a new chunk\n",
        "        if not current:\n",
        "            current = piece\n",
        "            continue\n",
        "\n",
        "        # If adding this piece stays within chunk_size, add it\n",
        "        if len(current) + 1 + len(piece) <= chunk_size:\n",
        "            current = current + \" \" + piece\n",
        "        else:\n",
        "            # Close current chunk\n",
        "            chunks.append(current.strip())\n",
        "\n",
        "            # Start new chunk with overlap from the end of previous chunk\n",
        "            if chunk_overlap > 0 and len(current) > chunk_overlap:\n",
        "                overlap = current[-chunk_overlap:]\n",
        "                current = overlap + \" \" + piece\n",
        "            else:\n",
        "                current = piece\n",
        "\n",
        "    if current:\n",
        "        chunks.append(current.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "#Just the helper function don't get confused by the name, if you want to change the chunk size do it here and not the above function\n",
        "def semantic_chunk_document(\n",
        "    text: str,\n",
        "    chunk_size: int = 500,\n",
        "    chunk_overlap: int =0,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Wrapper around recursive character-based chunking.\n",
        "\n",
        "    - Uses hierarchical separators: [\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \", \"\"]\n",
        "    - Respects `chunk_size` in characters\n",
        "    - Adds `chunk_overlap` (characters) between chunks\n",
        "    - No embeddings / similarity involved in chunking.\n",
        "    \"\"\"\n",
        "    return recursive_character_split(\n",
        "        text=text,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\"],\n",
        "    )"
      ],
      "metadata": {
        "id": "KLdsS8U6eUYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Indexing the context doc and storing it using Chroma DB\n",
        "\n",
        "def build_chroma_index(docs_folder: str = DOCS_FOLDER) -> None:\n",
        "    \"\"\"\n",
        "    - Load PDFs from folder\n",
        "    - Chunk each doc with recursive character-based chunker\n",
        "    - Embed chunks\n",
        "    - Store (id, document, embedding, metadata) in Chroma\n",
        "    \"\"\"\n",
        "    print(f\"[INFO] Building Chroma index from PDFs in: {docs_folder}\")\n",
        "\n",
        "    chroma_client: PersistentClient = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "\n",
        "    # Delete existing collection if we want a fresh rebuild\n",
        "    try:\n",
        "        chroma_client.delete_collection(COLLECTION_NAME)\n",
        "        print(f\"[INFO] Deleted existing collection '{COLLECTION_NAME}'.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
        "\n",
        "    pdf_docs = load_pdfs_from_folder(docs_folder)\n",
        "    if not pdf_docs:\n",
        "        print(\"[WARN] No PDFs found in docs folder.\")\n",
        "        return\n",
        "\n",
        "    all_ids: List[str] = []\n",
        "    all_texts: List[str] = []\n",
        "    all_embeddings: List[List[float]] = []\n",
        "    all_metadatas: List[Dict[str, Any]] = []\n",
        "\n",
        "    for doc_idx, (filename, text) in enumerate(pdf_docs):\n",
        "        print(f\"[INFO] Chunking document: {filename}\")\n",
        "        chunks = semantic_chunk_document(text)\n",
        "\n",
        "        if not chunks:\n",
        "            print(f\"[WARN] No chunks created for: {filename}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"       -> {len(chunks)} chunks\")\n",
        "\n",
        "        # Embed final chunks\n",
        "        chunk_embeddings = embed_texts(chunks)\n",
        "\n",
        "        for chunk_idx, (chunk_text, emb) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "            doc_id = f\"{doc_idx}-{chunk_idx}-{filename}\"\n",
        "            all_ids.append(doc_id)\n",
        "            all_texts.append(chunk_text)\n",
        "            all_embeddings.append(emb)\n",
        "            all_metadatas.append(\n",
        "                {\n",
        "                    \"source\": filename,\n",
        "                    \"chunk_index\": chunk_idx,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    if not all_texts:\n",
        "        print(\"[WARN] No chunks produced; nothing to index.\")\n",
        "        return\n",
        "\n",
        "    collection.add(\n",
        "        ids=all_ids,\n",
        "        documents=all_texts,\n",
        "        embeddings=all_embeddings,\n",
        "        metadatas=all_metadatas,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"[INFO] Indexed {len(all_texts)} chunks \"\n",
        "        f\"from {len(pdf_docs)} PDF(s) into collection '{COLLECTION_NAME}'.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_chroma_collection():\n",
        "    chroma_client: PersistentClient = chromadb.PersistentClient(path=CHROMA_PATH)\n",
        "    return chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n"
      ],
      "metadata": {
        "id": "Uz-fzjzseW2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retriver that retrieves the top k chunks\n",
        "\n",
        "def retrieve_top_k_chunks(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    - Embed query using SentenceTransformer\n",
        "    - Fetch all chunks + embeddings from Chroma\n",
        "    - Compute cosine similarity manually\n",
        "    - Return top-k chunks\n",
        "    \"\"\"\n",
        "    collection = get_chroma_collection()\n",
        "\n",
        "    data = collection.get(include=[\"documents\", \"embeddings\", \"metadatas\"])\n",
        "\n",
        "    docs = data.get(\"documents\", [])\n",
        "    embeddings = data.get(\"embeddings\", [])\n",
        "    metadatas = data.get(\"metadatas\", [])\n",
        "    ids = data.get(\"ids\", [])\n",
        "\n",
        "    if not docs:\n",
        "        print(\"[WARN] No documents in collection. Build the index first.\")\n",
        "        return []\n",
        "\n",
        "    query_emb = embed_texts([query])[0]\n",
        "\n",
        "    sims = [cosine_similarity(query_emb, emb) for emb in embeddings]\n",
        "\n",
        "    top_indices = sorted(range(len(sims)), key=lambda i: sims[i], reverse=True)[:k]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for idx in top_indices:\n",
        "        results.append(\n",
        "            {\n",
        "                \"id\": ids[idx] if idx < len(ids) else str(idx),\n",
        "                \"document\": docs[idx],\n",
        "                \"metadata\": metadatas[idx],\n",
        "                \"similarity\": sims[idx],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "WRgixysyeZHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining the context retrieved with a helpful prompt for gemma\n",
        "\n",
        "def build_prompt(query: str, retrieved_chunks: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"Build a simple context + question prompt for Gemma.\"\"\"\n",
        "    context_blocks = []\n",
        "    for i, item in enumerate(retrieved_chunks):\n",
        "        source = item[\"metadata\"].get(\"source\", \"unknown\")\n",
        "        context_blocks.append(\n",
        "            f\"[Chunk {i+1} | Source: {source}]\\n{item['document']}\"\n",
        "        )\n",
        "\n",
        "    context_str = \"\\n\\n\".join(context_blocks) if context_blocks else \"No context.\"\n",
        "\n",
        "    prompt = f\"\"\"You are ShaastraBot â€” an enthusiastic, helpful assistant for Shaastra IIT Madras!\n",
        "\n",
        "    Your job is to answer user questions **using ONLY the information found in the provided context**.\n",
        "    Do NOT use outside knowledge. If something is missing from the context, clearly say that it is not available.\n",
        "\n",
        "    When the user asks about an event:\n",
        "    - Try to include **date, time, venue, on-spot registration info, viewership status, prize details**, or any other specifics IF they appear anywhere in the context.\n",
        "    - Present the information in a friendly, energetic tone â€” you're excited to help participants learn about Shaastra!\n",
        "    - Keep the answer concise but informative.\n",
        "\n",
        "    General rules:\n",
        "    - NEVER hallucinate or invent details.\n",
        "    - If the context contains conflicting information, state that clearly.\n",
        "    - If the answer cannot be determined from the context, say so politely.\n",
        "    - Structure answers clearly using bullet points or short paragraphs when appropriate.\n",
        "    - If the userâ€™s query is broad or unclear, summarize relevant info from the context to help them.\n",
        "\n",
        "    Now use the following context to answer the userâ€™s question as accurately and enthusiastically as possible.\n",
        "\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "9KKjKRqSebL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using gemma locally to generate the answer\n",
        "def call_gemma_local(\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.2,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer using the local Gemma model.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Only decode the newly generated tokens, not the whole prompt\n",
        "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "v7vfMjaLedYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The helper function that uses everything sequentially to generate the answer\n",
        "\n",
        "def rag_answer(\n",
        "    query: str,\n",
        "    k: int = 5,\n",
        "    max_new_tokens: int = 512,\n",
        "    temperature: float = 0.2,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Full RAG pipeline:\n",
        "    - Embed query\n",
        "    - Retrieve top-k chunks from Chroma via cosine similarity\n",
        "    - Print chunks clearly\n",
        "    - Build prompt with context\n",
        "    - Call local Gemma\n",
        "    - Print & return answer\n",
        "    \"\"\"\n",
        "    chunks = retrieve_top_k_chunks(query, k=k)\n",
        "    if not chunks:\n",
        "        return \"[ERROR] No chunks retrieved. Did you build the index?\"\n",
        "\n",
        "    # Printing the Top K chunks for a better understanding while making the RAG pipeline\n",
        "    print(\"\\n================ RETRIEVED TOP CHUNKS ================\")\n",
        "    for i, c in enumerate(chunks, start=1):\n",
        "        print(f\"\\nðŸ”¹ Chunk {i}  (score={c['similarity']:.3f})\")\n",
        "        print(f\"Source: {c['metadata'].get('source')}\")\n",
        "        print(\"-\" * 60)\n",
        "        text_preview = c[\"document\"][:500].strip()\n",
        "        print(text_preview + (\"...\" if len(c[\"document\"]) > 500 else \"\"))\n",
        "    print(\"\\n========================================================\")\n",
        "\n",
        "    prompt = build_prompt(query, chunks)\n",
        "\n",
        "    print(\"\\n[INFO] Generating answer using local Gemma...\\n\")\n",
        "    answer = call_gemma_local(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    #Printing the final answer\n",
        "    print(\"\\n===================== ANSWER ==========================\\n\")\n",
        "    print(answer)\n",
        "    print(\"\\n========================================================\\n\")\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "print(\"[INFO] RAG setup complete. Steps:\")\n",
        "print(\"1) Upload PDFs into /content/docs (or change DOCS_FOLDER).\")\n",
        "print(\"2) Run: build_chroma_index().\")\n",
        "print(\"3) Run: rag_answer('your question').\")"
      ],
      "metadata": {
        "id": "5wrOc2uHefYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJxVqDgTZEvS",
        "outputId": "8002065f-ee51-4809-c163-22643848c04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Building Chroma index from PDFs in: /content/Documents\n",
            "[INFO] Deleted existing collection 'pdf_docs_collection'.\n",
            "[INFO] Loaded PDF: ShaastraContextDoc.pdf (45 pages)\n",
            "[INFO] Chunking document: ShaastraContextDoc.pdf\n",
            "       -> 191 chunks\n",
            "[INFO] Indexed 191 chunks from 1 PDF(s) into collection 'pdf_docs_collection'.\n"
          ]
        }
      ],
      "source": [
        "#Calling the function that chunks the context doc\n",
        "\n",
        "build_chroma_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPVS622MZjRN"
      },
      "outputs": [],
      "source": [
        "#Ask your questions here, feel free to mess around with the k value to get a better answer\n",
        "\n",
        "answer = rag_answer(\"What is AT-Makeathon?\", k=3)\n",
        "print(answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNVX6EfZRuzjT6enp93kXz3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}